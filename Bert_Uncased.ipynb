{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp6nh-Th0LNp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"/content/drive/MyDrive/JOKER/joker-2024-task2-classification-train (1).json\")\n",
        "df_test=pd.read_json(\"/content/drive/MyDrive/JOKER/joker-2024-task2-classification-test.json\")\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "PC4P66MP0OqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# Assuming df is your DataFrame with 'text' and 'class' columns\n",
        "X = df['text'].values\n",
        "y = df['class'].values\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Tokenize the input text\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
        "# Create TensorFlow datasets for training and testing\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "dict(train_encodings),\n",
        "y_train\n",
        ")).shuffle(len(X_train)).batch(8)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "dict(test_encodings),\n",
        "y_test\n",
        ")).batch(8)\n",
        "# Load the BERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "# Compile the model with sparse categorical crossentropy loss\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(train_dataset, epochs=3)\n",
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(test_dataset)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "# Make predictions\n",
        "predictions = model.predict(test_dataset)\n",
        "predicted_labels = label_encoder.inverse_transform(tf.argmax(predictions.logits, axis=1))\n",
        "# Generate classification report\n",
        "print(classification_report(label_encoder.inverse_transform(y_test), predicted_labels))\n"
      ],
      "metadata": {
        "id": "WptO1DiD0R17"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}